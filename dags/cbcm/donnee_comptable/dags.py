from datetime import timedelta

from airflow.decorators import dag
from airflow.models.baseoperator import chain
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

from infra.mails.default_smtp import create_airflow_callback, MailStatus

from utils.config.types import DagStatus
from utils.tasks.sql import (
    create_tmp_tables,
    import_file_to_db,
    copy_tmp_table_to_real_table,
    delete_tmp_tables,
    ensure_partition,
    LoadStrategy,
    create_projet_snapshot,
    get_projet_snapshot,
    refresh_views,
    # set_dataset_last_update_date,
)
from utils.tasks.s3 import (
    copy_s3_files,
    del_s3_files,
)
from utils.config.tasks import get_s3_keys_source, get_projet_config
from utils.config.dag_params import create_default_args, create_dag_params

from dags.cbcm.donnee_comptable.tasks import (
    source_files,
    validate_params,
    add_new_sp,
    get_sp,
    ajout_sp,
    datasets_additionnels,
)


# Mails
nom_projet = "Données comptable"
LINK_DOC_PIPELINE = "Non-défini"  # noqa
LINK_DOC_DATA = "Non-défini"  # noqa


# Définition du DAG
@dag(
    dag_id="chorus_donnees_comptables",
    schedule="*/15 8-19 * * 1-5",
    max_active_runs=1,
    max_consecutive_failed_dag_runs=1,
    catchup=False,
    tags=["CBCM", "DEV", "CHORUS"],
    description="Traitement des données comptables issues de Chorus",  # noqa
    default_args=create_default_args(),
    params=create_dag_params(
        nom_projet=nom_projet,
        dag_status=DagStatus.DEV,
        prod_schema="donnee_comptable",
        mail_enable=False,
        lien_pipeline=LINK_DOC_PIPELINE,
        lien_donnees=LINK_DOC_DATA,
    ),
    on_failure_callback=create_airflow_callback(
        mail_status=MailStatus.ERROR,
    ),
)
def chorus_donnees_comptables() -> None:
    """Task definition"""
    looking_for_files = S3KeySensor(
        task_id="looking_for_files",
        aws_conn_id="minio_bucket_dsci",
        bucket_name="dsci",
        bucket_key=get_s3_keys_source(nom_projet=nom_projet),
        mode="reschedule",
        poke_interval=timedelta(seconds=30),
        timeout=timedelta(minutes=13),
        soft_fail=True,
        on_skipped_callback=create_airflow_callback(mail_status=MailStatus.SKIP),
        on_success_callback=create_airflow_callback(mail_status=MailStatus.START),
    )

    # Ordre des tâches
    chain(
        validate_params(),
        looking_for_files,
        create_projet_snapshot(nom_projet=nom_projet),
        get_projet_snapshot(nom_projet=nom_projet),
        source_files(),
        add_new_sp(),
        get_sp(),
        ajout_sp(),
        datasets_additionnels(),
        create_tmp_tables(reset_id_seq=False),
        import_file_to_db.expand(
            selecteur_config=get_projet_config(nom_projet=nom_projet)
        ),
        ensure_partition(),
        copy_tmp_table_to_real_table(
            load_strategy=LoadStrategy.APPEND,
        ),
        refresh_views(),
        copy_s3_files(bucket="dsci"),
        del_s3_files(bucket="dsci"),
        delete_tmp_tables(),
        # set_dataset_last_update_date(
        #     dataset_ids=[49, 50, 51, 52, 53, 54],
        # ),
    )


chorus_donnees_comptables()
